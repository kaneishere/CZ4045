{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9ee24a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import spacy\n",
    "\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15500aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'data/reviewSelected100.json'\n",
    "os.path.exists(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d25f8f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8aoJJdKEO3ypoZNszpPu7Q</td>\n",
       "      <td>bGgAL09pxLnV_FFgR4ZADg</td>\n",
       "      <td>ZBE-H_aUlicix_9vUGQPIQ</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We had my Mother's Birthday Party here on 10/2...</td>\n",
       "      <td>2016-11-09 20:07:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J5NOCLdhuhor7USRhtYZ8w</td>\n",
       "      <td>pFCb-1j6oI3TDjr26h2cJQ</td>\n",
       "      <td>e-YnECeZNt8ngm0tu4X9mQ</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good Korean grill near Eaton Centre. The marin...</td>\n",
       "      <td>2015-12-05 05:06:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PXiLWAYRt3xnHaJ8MB4rzw</td>\n",
       "      <td>mEzc6LeTNiQgIVsq3poMbg</td>\n",
       "      <td>j7HO1YeMQGYo3KibMXZ5vg</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Was recommended to try this place by few peopl...</td>\n",
       "      <td>2014-10-11 05:16:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VrLarvxZYJm74yAqtpe9PQ</td>\n",
       "      <td>o-zUN2WEZgjQS7jnNsec0g</td>\n",
       "      <td>7e3PZzUpG5FYOTGt3O3ePA</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ambience: Would not expect something this nice...</td>\n",
       "      <td>2016-07-25 03:45:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1CUpidlVFprUCkApqzCmA</td>\n",
       "      <td>Wlx0iBXJvk4x0EeOt2Bz1Q</td>\n",
       "      <td>vuHzLZ7nAeT-EiecOkS5Og</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Absolutely the WORST pool company that I have ...</td>\n",
       "      <td>2016-04-11 18:49:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  8aoJJdKEO3ypoZNszpPu7Q  bGgAL09pxLnV_FFgR4ZADg  ZBE-H_aUlicix_9vUGQPIQ   \n",
       "1  J5NOCLdhuhor7USRhtYZ8w  pFCb-1j6oI3TDjr26h2cJQ  e-YnECeZNt8ngm0tu4X9mQ   \n",
       "2  PXiLWAYRt3xnHaJ8MB4rzw  mEzc6LeTNiQgIVsq3poMbg  j7HO1YeMQGYo3KibMXZ5vg   \n",
       "3  VrLarvxZYJm74yAqtpe9PQ  o-zUN2WEZgjQS7jnNsec0g  7e3PZzUpG5FYOTGt3O3ePA   \n",
       "4  C1CUpidlVFprUCkApqzCmA  Wlx0iBXJvk4x0EeOt2Bz1Q  vuHzLZ7nAeT-EiecOkS5Og   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      5       0      0     0   \n",
       "1      4       0      0     0   \n",
       "2      5       2      1     3   \n",
       "3      3       0      0     0   \n",
       "4      1      11      0     3   \n",
       "\n",
       "                                                text                date  \n",
       "0  We had my Mother's Birthday Party here on 10/2... 2016-11-09 20:07:25  \n",
       "1  Good Korean grill near Eaton Centre. The marin... 2015-12-05 05:06:43  \n",
       "2  Was recommended to try this place by few peopl... 2014-10-11 05:16:15  \n",
       "3  Ambience: Would not expect something this nice... 2016-07-25 03:45:26  \n",
       "4  Absolutely the WORST pool company that I have ... 2016-04-11 18:49:11  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df = pd.read_json(review_path, lines=True,encoding = \"ISO-8859-1\")\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f88ba98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Generate five random indexes'\n",
    "review_random_idx = np.array(np.random.rand(5)*len(review_df), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "88774c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_random_df = review_df.iloc[review_random_idx]\n",
    "review_random_df = review_random_df_1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "69720ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Very friendly staff! Took my kitten Ryuk in to...\n",
      "1    Our family placed an order.  Not only did the ...\n",
      "2    We love Trader Joe's, wish parking was better ...\n",
      "3    Between the knowledgeable staff and amazing pr...\n",
      "4    Went twice this week and want to go again.  Fo...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(review_random_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "243d0d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Very friendly staff! Took my kitten Ryuk in today for a fix and shots. Pricing was fair. Even trimmed his nails. No \"visit fee\" added. Ryuk saw Talia Gattenuo, she was very friendly and up front about any charges and fees. The price she gave me in the AM was the same when I picked him up! She even let me know my baby is super cuddly (which I know) but that just shows me they don't just shove my baby into a cage and ignore him. \n",
      "\n",
      "Would recommend.\n"
     ]
    }
   ],
   "source": [
    "print(type(review_random_df['text'].loc[0]))\n",
    "print(review_random_df['text'].loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d65305",
   "metadata": {},
   "source": [
    "# Method 1 using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "242dc8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/guangxushen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/guangxushen/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a79ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Tokenize the text and Tag the Part-Of-Speech of each words'\n",
    "review_random_df_1['tokenize'] = review_random_df['text'].apply(nltk.word_tokenize)\n",
    "review_random_df_1['pos_tag'] = review_random_df_1['tokenize'].apply(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "915e135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Output the resulting json '\n",
    "review_random_df_1.to_json(r'output/reviewTagging5_1.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7707b74",
   "metadata": {},
   "source": [
    "# Method 2 using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1517b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "86223aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_disk(self, path, exclude=tuple()):\n",
    "    # This will receive the directory path + /my_component\n",
    "    data_path = path / \"data.json\"\n",
    "    with data_path.open(\"w\", encoding=\"utf8\") as f:\n",
    "        f.write(json.dumps(self.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4ac33a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(review_random_df['text'].to_string())\n",
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e19efe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Very friendly staff! Took my kitten Ryuk in to...\n",
      "1    Our family placed an order.  Not only did the ...\n",
      "2    We love Trader Joe's, wish parking was better ...\n",
      "3    Between the knowledgeable staff and amazing pr...\n",
      "4    Went twice this week and want to go again.  Fo...\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "sentences = list(doc.sents)\n",
    "print(len(sentences))\n",
    "# doc.to_json(r'output/reviewTagging5_2_doc.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "70bbb1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 PUNCT\n",
      "    SPACE\n",
      "Very ADV\n",
      "friendly ADJ\n",
      "staff NOUN\n",
      "! PUNCT\n",
      "Took VERB\n",
      "my DET\n",
      "kitten NOUN\n",
      "Ryuk PROPN\n",
      "in ADP\n",
      "to ADP\n",
      "... PUNCT\n",
      "\n",
      " SPACE\n",
      "1 X\n",
      "    SPACE\n",
      "Our DET\n",
      "family NOUN\n",
      "placed VERB\n",
      "an DET\n",
      "order NOUN\n",
      ". PUNCT\n",
      "  SPACE\n",
      "Not PART\n",
      "only ADV\n",
      "did AUX\n",
      "the DET\n",
      "... PUNCT\n",
      "\n",
      " SPACE\n",
      "2 X\n",
      "    SPACE\n",
      "We PRON\n",
      "love VERB\n",
      "Trader PROPN\n",
      "Joe PROPN\n",
      "'s PART\n",
      ", PUNCT\n",
      "wish ADJ\n",
      "parking NOUN\n",
      "was AUX\n",
      "better ADJ\n",
      "... PUNCT\n",
      "\n",
      " SPACE\n",
      "3 X\n",
      "    SPACE\n",
      "Between ADP\n",
      "the DET\n",
      "knowledgeable ADJ\n",
      "staff NOUN\n",
      "and CCONJ\n",
      "amazing ADJ\n",
      "pr NOUN\n",
      "... PUNCT\n",
      "\n",
      " SPACE\n",
      "4 X\n",
      "    SPACE\n",
      "Went VERB\n",
      "twice ADV\n",
      "this DET\n",
      "week NOUN\n",
      "and CCONJ\n",
      "want VERB\n",
      "to PART\n",
      "go VERB\n",
      "again ADV\n",
      ". PUNCT\n",
      "  SPACE\n",
      "Fo PROPN\n",
      "... PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4997d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_random_df_2 = review_df.iloc[review_random_idx]\n",
    "#review_random_df_2 = review_random_df_2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "389d3a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert series object to string\n",
    "# review_random_df_2['text'] = review_random_df_2['text'].astype(str)\n",
    "# print(review_random_df_2['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "492a93be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Tokenize the text and Tag the Part-Of-Speech of each words'\n",
    "# review_random_df_2['tokenize'] = tokenizer(review_random_df_2['text'].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "97948d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp(review_random_df_2['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e32eafc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(sents, num):\n",
    "    words = dict.fromkeys((i for i in range(num)), [])\n",
    "    for i in range(num):\n",
    "        words[i] = word_tokenize(sents[i])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b601809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron tagger trained on Wall Street Journal corpus\n",
    "def perceptron_tagger(words, num):\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    pos_1 = dict.fromkeys((i for i in range(num)), [])\n",
    "    for i,j in words.items():\n",
    "        pos_1[i] = nltk.pos_tag(j)\n",
    "    return pos_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d09f34ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Very friendly staff! Took my kitten Ryuk in to...\n",
      "1    Our family placed an order.  Not only did the ...\n",
      "2    We love Trader Joe's, wish parking was better ...\n",
      "3    Between the knowledgeable staff and amazing pr...\n",
      "4    Went twice this week and want to go again.  Fo...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(review_random_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "29312386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['Very', 'friendly', 'staff', '!', 'Took', 'my', 'kitten', 'Ryuk', 'in', 'today', 'for', 'a', 'fix', 'and', 'shots', '.', 'Pricing', 'was', 'fair', '.', 'Even', 'trimmed', 'his', 'nails', '.', 'No', '``', 'visit', 'fee', \"''\", 'added', '.', 'Ryuk', 'saw', 'Talia', 'Gattenuo', ',', 'she', 'was', 'very', 'friendly', 'and', 'up', 'front', 'about', 'any', 'charges', 'and', 'fees', '.', 'The', 'price', 'she', 'gave', 'me', 'in', 'the', 'AM', 'was', 'the', 'same', 'when', 'I', 'picked', 'him', 'up', '!', 'She', 'even', 'let', 'me', 'know', 'my', 'baby', 'is', 'super', 'cuddly', '(', 'which', 'I', 'know', ')', 'but', 'that', 'just', 'shows', 'me', 'they', 'do', \"n't\", 'just', 'shove', 'my', 'baby', 'into', 'a', 'cage', 'and', 'ignore', 'him', '.', 'Would', 'recommend', '.'], 1: ['Our', 'family', 'placed', 'an', 'order', '.', 'Not', 'only', 'did', 'the', 'order', 'take', '45', 'minutes', 'but', 'it', 'was', 'literally', 'served', 'to', 'other', 'folks', 'across', 'from', 'us', '.', 'Thankfully', 'it', 'did', \"n't\", 'go', 'unnoticed', '.', 'Food', 'was', 'contaminated', ',', 'picked', 'up', 'by', 'one', 'server', 'and', 'taken', 'to', 'the', 'back', 'and', 'brought', 'back', 'out', 'by', 'a', 'different', 'server', '.', 'GROSS', 'The', 'food', 'was', 'literally', 'going', 'to', 'be', 'served', 'to', 'us', 'until', 'I', 'mentioned', 'what', 'had', 'happened', '.', 'I', 'was', \"n't\", 'given', 'the', 'option', 'to', 'have', 'a', 'whole', 'new', 'badge', 'of', 'food', 'made', 'instead', 'I', 'had', 'to', 'ask', '.', 'I', 'felt', 'uneasy', 'so', 'decided', 'to', 'let', 'management', 'know', 'we', 'were', 'leaving', '.', 'One', 'star', 'it', 'too', 'much', '...'], 2: ['We', 'love', 'Trader', 'Joe', \"'s\", ',', 'wish', 'parking', 'was', 'better', 'but', 'what', 'can', 'you', 'do', 'Monroe', 'Street', '.', 'Its', 'also', 'a', 'smaller', 'space', 'for', 'parking', 'and', 'in', 'the', 'store', 'in', 'general', '.', 'I', 'try', 'to', 'avoid', 'the', 'place', 'on', 'weekends', '.', 'The', 'staff', 'are', 'always', 'friendly', 'and', 'helpful', '.', 'I', 'like', 'that', 'they', 'have', 'affordable', 'organic', 'produce', '.', 'They', 'also', 'have', 'a', 'nice', 'selection', 'of', 'different', 'foods', 'you', 'might', 'not', 'find', 'other', 'places', '.', 'I', 'really', 'like', 'a', 'lot', 'if', 'their', 'frozen', ',', 'prepackaged', 'meals', 'like', 'their', 'vegan', 'and', 'non', 'vegan', 'chicken', 'tikka', 'masala', '.', 'I', 'hear', 'their', 'Mac', 'and', 'cheese', 'is', 'good', 'but', 'have', 'not', 'tried', 'it', 'myself', '.', 'Its', 'nice', 'when', 'I', \"'m\", 'on', 'the', 'go', 'and', 'then', 'I', 'dont', 'feel', 'too', 'guilty', 'about', 'eating', 'a', 'frozen', 'meal', 'as', 'I', 'trust', 'their', 'quality', 'and', 'they', 'have', 'a', 'good', 'variety', 'of', 'healthier', 'options', '.', 'I', 'also', 'love', 'their', 'frozen', 'veggies', 'section', '.', 'Lots', 'of', 'nice', 'organic', 'choices', '.', 'They', 'have', 'a', 'good', 'variety', 'of', 'frozen', 'fruit', 'for', 'my', 'smoothies', 'too', '.', 'I', 'love', 'the', 'pre-cooked', 'frozen', 'edamame', 'for', 'my', 'son', '.', 'They', 'also', 'have', 'soy', 'and', 'coconut', 'ice', 'cream', 'that', 'is', 'better', 'than', 'most', 'other', 'brands', 'out', 'there', 'and', 'it', 'is', 'super', 'affordable', '.'], 3: ['Between', 'the', 'knowledgeable', 'staff', 'and', 'amazing', 'prices', ',', 'this', 'place', 'literally', 'makes', 'you', 'feel', 'safe', 'in', 'this', 'over', 'saturated', 'vapor', 'market', '.', 'I', 'feel', 'like', 'there', 'are', 'now', 'more', 'vapor', 'stores', 'than', 'McDonald', \"'s\", 'around', 'town', ',', 'with', 'that', 'comes', 'very', 'inconsistent', 'treatment', ',', 'prices', ',', 'and', 'information', '.', 'Trust', 'me', 'when', 'I', 'say', 'that', 'this', 'should', 'be', 'your', 'one', '``', 'go-to', \"''\", 'place', 'for', 'any', 'of', 'your', 'vapor', 'needs', '.', 'I', 'would', 'like', 'to', 'commend', 'Scott', '(', 'the', 'owner', ')', 'for', 'his', 'proactive', 'approach', 'of', 'always', 'being', 'close', 'enough', 'to', 'his', 'business', 'to', 'instill', 'such', 'good', 'customer', 'treatment', 'to', 'his', 'employees', ',', 'and', 'for', 'hiring', 'Shane', 'as', 'a', 'manager', '.', 'With', 'the', 'two', 'of', 'them', 'there', ',', 'the', 'place', 'is', 'a', 'well', 'oiled', 'machine', ',', 'and', 'it', 'will', 'have', 'my', 'complete', 'loyalty', 'throughout', 'my', 'vaping', 'life', '.', 'They', 'have', 'even', 'helped', 'remedy', 'situations', 'that', 'OTHER', 'vapor', 'stores', 'messed', 'up', 'on', 'to', 'earn', 'my', 'business', 'with', 'them', '.', '10', 'out', 'of', '10', 'on', 'everything', '.', 'Keep', 'up', 'the', 'great', 'work', 'guys', '!', 'Lucas', 'Barcelo'], 4: ['Went', 'twice', 'this', 'week', 'and', 'want', 'to', 'go', 'again', '.', 'Food', 'is', 'fresh', 'and', 'a', 'great', 'variety', 'of', 'flavors', '.', 'I', 'highly', 'recommend', 'their', 'house', 'sauce', '.', 'So', 'glad', 'this', 'place', 'opened', 'close', 'to', 'home', '.']}\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenizer(review_random_df['text'], 5)\n",
    "#perceptron_pos = perceptron_tagger(review_random_df['text'], 5)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ec6e0056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/guangxushen/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cd09170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram tagger trained by Brown corpus\n",
    "def unigram_tagger(words, num):\n",
    "    from nltk.corpus import brown\n",
    "    brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "    brown_sents = brown.sents(categories='news')\n",
    "    #print(\"=======brown_sents=====\")\n",
    "    #print(brown_sents)\n",
    "    ut = nltk.UnigramTagger(brown_tagged_sents)\n",
    "    #print(\"------ut---------\")\n",
    "    #print(ut)\n",
    "    pos_2 = dict.fromkeys((i for i in range(num)), [])\n",
    "    for i,j in words.items():\n",
    "        pos_2[i] = ut.tag(j)\n",
    "    return pos_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2a26d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram_pos = unigram_tagger(words, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a3e808bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results of unigram tagger:\n",
      "[('Very', 'QL'), ('friendly', 'JJ'), ('staff', 'NN'), ('!', '.'), ('Took', None), ('my', 'PP$'), ('kitten', None), ('Ryuk', None), ('in', 'IN'), ('today', 'NR'), ('for', 'IN'), ('a', 'AT'), ('fix', None), ('and', 'CC'), ('shots', 'NNS'), ('.', '.'), ('Pricing', None), ('was', 'BEDZ'), ('fair', 'JJ'), ('.', '.'), ('Even', 'RB'), ('trimmed', 'VBN'), ('his', 'PP$'), ('nails', None), ('.', '.'), ('No', 'AT'), ('``', '``'), ('visit', 'NN'), ('fee', 'NN'), (\"''\", \"''\"), ('added', 'VBD'), ('.', '.'), ('Ryuk', None), ('saw', 'VBD'), ('Talia', None), ('Gattenuo', None), (',', ','), ('she', 'PPS'), ('was', 'BEDZ'), ('very', 'QL'), ('friendly', 'JJ'), ('and', 'CC'), ('up', 'RP'), ('front', 'NN'), ('about', 'IN'), ('any', 'DTI'), ('charges', 'NNS'), ('and', 'CC'), ('fees', 'NNS'), ('.', '.'), ('The', 'AT'), ('price', 'NN'), ('she', 'PPS'), ('gave', 'VBD'), ('me', 'PPO'), ('in', 'IN'), ('the', 'AT'), ('AM', 'NN'), ('was', 'BEDZ'), ('the', 'AT'), ('same', 'AP'), ('when', 'WRB'), ('I', 'PPSS'), ('picked', 'VBD'), ('him', 'PPO'), ('up', 'RP'), ('!', '.'), ('She', 'PPS'), ('even', 'RB'), ('let', 'VB'), ('me', 'PPO'), ('know', 'VB'), ('my', 'PP$'), ('baby', 'NN'), ('is', 'BEZ'), ('super', 'JJ'), ('cuddly', None), ('(', '('), ('which', 'WDT'), ('I', 'PPSS'), ('know', 'VB'), (')', ')'), ('but', 'CC'), ('that', 'CS'), ('just', 'RB'), ('shows', 'NNS'), ('me', 'PPO'), ('they', 'PPSS'), ('do', 'DO'), (\"n't\", None), ('just', 'RB'), ('shove', None), ('my', 'PP$'), ('baby', 'NN'), ('into', 'IN'), ('a', 'AT'), ('cage', None), ('and', 'CC'), ('ignore', None), ('him', 'PPO'), ('.', '.'), ('Would', 'MD-HL'), ('recommend', 'VB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResults of unigram tagger:\")\n",
    "print(unigram_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3bfd03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88824abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
