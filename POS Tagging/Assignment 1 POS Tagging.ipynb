{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9ee24a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import spacy\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "15500aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'data/reviewSelected100.json'\n",
    "os.path.exists(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d25f8f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8aoJJdKEO3ypoZNszpPu7Q</td>\n",
       "      <td>bGgAL09pxLnV_FFgR4ZADg</td>\n",
       "      <td>ZBE-H_aUlicix_9vUGQPIQ</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We had my Mother's Birthday Party here on 10/2...</td>\n",
       "      <td>2016-11-09 20:07:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J5NOCLdhuhor7USRhtYZ8w</td>\n",
       "      <td>pFCb-1j6oI3TDjr26h2cJQ</td>\n",
       "      <td>e-YnECeZNt8ngm0tu4X9mQ</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good Korean grill near Eaton Centre. The marin...</td>\n",
       "      <td>2015-12-05 05:06:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PXiLWAYRt3xnHaJ8MB4rzw</td>\n",
       "      <td>mEzc6LeTNiQgIVsq3poMbg</td>\n",
       "      <td>j7HO1YeMQGYo3KibMXZ5vg</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Was recommended to try this place by few peopl...</td>\n",
       "      <td>2014-10-11 05:16:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VrLarvxZYJm74yAqtpe9PQ</td>\n",
       "      <td>o-zUN2WEZgjQS7jnNsec0g</td>\n",
       "      <td>7e3PZzUpG5FYOTGt3O3ePA</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ambience: Would not expect something this nice...</td>\n",
       "      <td>2016-07-25 03:45:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1CUpidlVFprUCkApqzCmA</td>\n",
       "      <td>Wlx0iBXJvk4x0EeOt2Bz1Q</td>\n",
       "      <td>vuHzLZ7nAeT-EiecOkS5Og</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Absolutely the WORST pool company that I have ...</td>\n",
       "      <td>2016-04-11 18:49:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  8aoJJdKEO3ypoZNszpPu7Q  bGgAL09pxLnV_FFgR4ZADg  ZBE-H_aUlicix_9vUGQPIQ   \n",
       "1  J5NOCLdhuhor7USRhtYZ8w  pFCb-1j6oI3TDjr26h2cJQ  e-YnECeZNt8ngm0tu4X9mQ   \n",
       "2  PXiLWAYRt3xnHaJ8MB4rzw  mEzc6LeTNiQgIVsq3poMbg  j7HO1YeMQGYo3KibMXZ5vg   \n",
       "3  VrLarvxZYJm74yAqtpe9PQ  o-zUN2WEZgjQS7jnNsec0g  7e3PZzUpG5FYOTGt3O3ePA   \n",
       "4  C1CUpidlVFprUCkApqzCmA  Wlx0iBXJvk4x0EeOt2Bz1Q  vuHzLZ7nAeT-EiecOkS5Og   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      5       0      0     0   \n",
       "1      4       0      0     0   \n",
       "2      5       2      1     3   \n",
       "3      3       0      0     0   \n",
       "4      1      11      0     3   \n",
       "\n",
       "                                                text                date  \n",
       "0  We had my Mother's Birthday Party here on 10/2... 2016-11-09 20:07:25  \n",
       "1  Good Korean grill near Eaton Centre. The marin... 2015-12-05 05:06:43  \n",
       "2  Was recommended to try this place by few peopl... 2014-10-11 05:16:15  \n",
       "3  Ambience: Would not expect something this nice... 2016-07-25 03:45:26  \n",
       "4  Absolutely the WORST pool company that I have ... 2016-04-11 18:49:11  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df = pd.read_json(data_path, lines=True,encoding = \"ISO-8859-1\")\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f88ba98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly generate five indexes\n",
    "review_random_idx = np.array(np.random.rand(5)*len(review_df), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "88774c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract out those sentences \n",
    "review_random_df = review_df.iloc[review_random_idx]\n",
    "review_random_df = review_random_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "69720ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Everyone at Auto Tint Express was super friend...\n",
      "1    For a waffle house this place is a little bett...\n",
      "2    I know that I'm in the minority... but I reall...\n",
      "3    The view is amazing but the food is all crap. ...\n",
      "4    We have used Happy Endings services twice now....\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(review_random_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "243d0d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everyone at Auto Tint Express was super friendly, they helped me pick a good shade to go with for my girls 2014 corolla. Everything came out amazing and the waiting room is super nice. Free drinks and TV to watch with a comfortable couch. Definitely check them out for any vehicle you have!\n"
     ]
    }
   ],
   "source": [
    "print(review_random_df['text'].loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d65305",
   "metadata": {},
   "source": [
    "# Method 1 using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "242dc8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/guangxushen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/guangxushen/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5a79ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the text\n",
    "review_random_df['tokenize'] = review_random_df['text'].apply(nltk.word_tokenize)\n",
    "#Tag those individual tokens respectively\n",
    "review_random_df['pos_tag'] = review_random_df['tokenize'].apply(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "915e135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the output as the resulting json\n",
    "review_random_df.to_json(r'output/reviewTagging5_1.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7707b74",
   "metadata": {},
   "source": [
    "# Method 2 using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1517b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4ac33a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(review_random_df['text'].to_string())\n",
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "801ea464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Everyone at Auto Tint Express was super friend...\n",
      "1    For a waffle house this place is a little bett...\n",
      "2    I know that I'm in the minority... but I reall...\n",
      "3    The view is amazing but the food is all crap. ...\n",
      "4    We have used Happy Endings services twice now....\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "sentences = list(doc.sents)\n",
    "print(len(sentences))\n",
    "# doc.to_json(r'output/reviewTagging5_2_doc.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "006d2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get each of the line ready for pos tagging\n",
    "doc_0 = nlp(review_random_df.loc[0]['text'])\n",
    "doc_1 = nlp(review_random_df.loc[1]['text'])\n",
    "doc_2 = nlp(review_random_df.loc[2]['text'])\n",
    "doc_3 = nlp(review_random_df.loc[3]['text'])\n",
    "doc_4 = nlp(review_random_df.loc[2]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8b61ebf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a waffle house this place is a little better than the others,, Staff is exceptional, and the place is always VERY Clean.... Awesome job...\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#checking of the sentence obtained\n",
    "print(doc_1)\n",
    "sentence_1 = list(doc_1.sents)\n",
    "print(len(sentence_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c4225c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everyone PRON\n",
      "at ADP\n",
      "Auto PROPN\n",
      "Tint PROPN\n",
      "Express PROPN\n",
      "was AUX\n",
      "super ADV\n",
      "friendly ADJ\n",
      ", PUNCT\n",
      "they PRON\n",
      "helped VERB\n",
      "me PRON\n",
      "pick VERB\n",
      "a DET\n",
      "good ADJ\n",
      "shade NOUN\n",
      "to PART\n",
      "go VERB\n",
      "with ADP\n",
      "for ADP\n",
      "my DET\n",
      "girls NOUN\n",
      "2014 NUM\n",
      "corolla NOUN\n",
      ". PUNCT\n",
      "Everything PRON\n",
      "came VERB\n",
      "out ADP\n",
      "amazing ADJ\n",
      "and CCONJ\n",
      "the DET\n",
      "waiting NOUN\n",
      "room NOUN\n",
      "is AUX\n",
      "super ADV\n",
      "nice ADJ\n",
      ". PUNCT\n",
      "Free ADJ\n",
      "drinks NOUN\n",
      "and CCONJ\n",
      "TV NOUN\n",
      "to PART\n",
      "watch VERB\n",
      "with ADP\n",
      "a DET\n",
      "comfortable ADJ\n",
      "couch NOUN\n",
      ". PUNCT\n",
      "Definitely ADV\n",
      "check VERB\n",
      "them PRON\n",
      "out ADP\n",
      "for ADP\n",
      "any DET\n",
      "vehicle NOUN\n",
      "you PRON\n",
      "have AUX\n",
      "! PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc_0:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "592d4002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ADP\n",
      "a DET\n",
      "waffle NOUN\n",
      "house NOUN\n",
      "this DET\n",
      "place NOUN\n",
      "is AUX\n",
      "a DET\n",
      "little ADJ\n",
      "better ADJ\n",
      "than SCONJ\n",
      "the DET\n",
      "others NOUN\n",
      ", PUNCT\n",
      ", PUNCT\n",
      "Staff PROPN\n",
      "is AUX\n",
      "exceptional ADJ\n",
      ", PUNCT\n",
      "and CCONJ\n",
      "the DET\n",
      "place NOUN\n",
      "is AUX\n",
      "always ADV\n",
      "VERY ADV\n",
      "Clean ADJ\n",
      ".... PUNCT\n",
      "Awesome ADJ\n",
      "job NOUN\n",
      "... PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc_1:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3ff2a8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "know VERB\n",
      "that SCONJ\n",
      "I PRON\n",
      "'m AUX\n",
      "in ADP\n",
      "the DET\n",
      "minority NOUN\n",
      "... PUNCT\n",
      "but CCONJ\n",
      "I PRON\n",
      "really ADV\n",
      "do AUX\n",
      "not PART\n",
      "like VERB\n",
      "The DET\n",
      "Pretzel PROPN\n",
      "Shop PROPN\n",
      ". PUNCT\n",
      "My DET\n",
      "co NOUN\n",
      "- NOUN\n",
      "workers NOUN\n",
      "often ADV\n",
      "pick VERB\n",
      "up ADP\n",
      "pretzels NOUN\n",
      "from ADP\n",
      "here ADV\n",
      "for ADP\n",
      "birthdays NOUN\n",
      "or CCONJ\n",
      "other ADJ\n",
      "office NOUN\n",
      "events NOUN\n",
      ", PUNCT\n",
      "and CCONJ\n",
      "everyone PRON\n",
      "spends VERB\n",
      "the DET\n",
      "morning NOUN\n",
      "raving VERB\n",
      "about ADP\n",
      "how ADV\n",
      "delicious ADJ\n",
      "the DET\n",
      "pretzels NOUN\n",
      "are AUX\n",
      "and CCONJ\n",
      "how ADV\n",
      "they PRON\n",
      "can VERB\n",
      "not PART\n",
      "stop VERB\n",
      "eating VERB\n",
      "them PRON\n",
      ". PUNCT\n",
      "Am AUX\n",
      "I PRON\n",
      "missing VERB\n",
      "something PRON\n",
      "? PUNCT\n",
      "I PRON\n",
      "just ADV\n",
      "sit VERB\n",
      "quietly ADV\n",
      "at ADP\n",
      "my DET\n",
      "desk NOUN\n",
      "and CCONJ\n",
      "raise VERB\n",
      "an DET\n",
      "eyebrow NOUN\n",
      "at ADP\n",
      "all DET\n",
      "this DET\n",
      "chatter NOUN\n",
      ". PUNCT\n",
      "I PRON\n",
      "'ve AUX\n",
      "had AUX\n",
      "them PRON\n",
      "at ADP\n",
      "least ADJ\n",
      "5 NUM\n",
      "- SYM\n",
      "8 NUM\n",
      "times NOUN\n",
      "and CCONJ\n",
      "can VERB\n",
      "not PART\n",
      "say VERB\n",
      "that SCONJ\n",
      "I PRON\n",
      "'ve AUX\n",
      "ever ADV\n",
      "enjoyed VERB\n",
      "them PRON\n",
      ". PUNCT\n",
      "The DET\n",
      "regular ADJ\n",
      "pretzels NOUN\n",
      "are AUX\n",
      "either CCONJ\n",
      "over ADP\n",
      "or CCONJ\n",
      "under ADP\n",
      "salted VERB\n",
      "( PUNCT\n",
      "not PART\n",
      "sure ADJ\n",
      "if SCONJ\n",
      "that DET\n",
      "is AUX\n",
      "a DET\n",
      "preference NOUN\n",
      "the DET\n",
      "customer NOUN\n",
      "can VERB\n",
      "select VERB\n",
      "or CCONJ\n",
      "if SCONJ\n",
      "it PRON\n",
      "'s AUX\n",
      "just ADV\n",
      "random ADJ\n",
      ") PUNCT\n",
      ", PUNCT\n",
      "and CCONJ\n",
      "the DET\n",
      "cinnamon PROPN\n",
      "sugar NOUN\n",
      "just ADV\n",
      "taste VERB\n",
      "strange ADJ\n",
      ". PUNCT\n",
      "They PRON\n",
      "often ADV\n",
      "taste VERB\n",
      "very ADV\n",
      "soggy ADJ\n",
      ", PUNCT\n",
      "too ADV\n",
      ", PUNCT\n",
      "like SCONJ\n",
      "they PRON\n",
      "were AUX\n",
      "bagged VERB\n",
      "when ADV\n",
      "still ADV\n",
      "too ADV\n",
      "warm ADJ\n",
      ". PUNCT\n",
      "Occasionally ADV\n",
      ", PUNCT\n",
      "co NOUN\n",
      "- NOUN\n",
      "workers NOUN\n",
      "pick VERB\n",
      "- PUNCT\n",
      "up ADP\n",
      "the DET\n",
      "\" PUNCT\n",
      "cheese NOUN\n",
      "\" PUNCT\n",
      "sauce NOUN\n",
      ". PUNCT\n",
      "This DET\n",
      "stuff NOUN\n",
      "looks VERB\n",
      "like SCONJ\n",
      "a DET\n",
      "melted ADJ\n",
      "yellow ADJ\n",
      "crayon NOUN\n",
      ". PUNCT\n",
      "I PRON\n",
      "love VERB\n",
      "all DET\n",
      "types NOUN\n",
      "of ADP\n",
      "cheese NOUN\n",
      ", PUNCT\n",
      "but CCONJ\n",
      "this DET\n",
      "looks VERB\n",
      "and CCONJ\n",
      "tastes VERB\n",
      "terrible ADJ\n",
      ". PUNCT\n",
      "Yes INTJ\n",
      ", PUNCT\n",
      "I PRON\n",
      "'m AUX\n",
      "being AUX\n",
      "blasphemous ADJ\n",
      "towards ADP\n",
      "a DET\n",
      "Pittsburgh PROPN\n",
      "tradition NOUN\n",
      ", PUNCT\n",
      "I PRON\n",
      "know VERB\n",
      ", PUNCT\n",
      "but CCONJ\n",
      "I PRON\n",
      "'m AUX\n",
      "sorry ADJ\n",
      ", PUNCT\n",
      "I PRON\n",
      "'d VERB\n",
      "much ADV\n",
      "rather ADV\n",
      "get AUX\n",
      "a DET\n",
      "pretzel NOUN\n",
      "from ADP\n",
      "Hofbrauhaus PROPN\n",
      "or CCONJ\n",
      "even ADV\n",
      "Auntie PROPN\n",
      "Anne PROPN\n",
      "'s PART\n",
      "in ADP\n",
      "the DET\n",
      "mall NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc_2:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1c0826a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ADP\n",
      "a DET\n",
      "waffle NOUN\n",
      "house NOUN\n",
      "this DET\n",
      "place NOUN\n",
      "is AUX\n",
      "a DET\n",
      "little ADJ\n",
      "better ADJ\n",
      "than SCONJ\n",
      "the DET\n",
      "others NOUN\n",
      ", PUNCT\n",
      ", PUNCT\n",
      "Staff PROPN\n",
      "is AUX\n",
      "exceptional ADJ\n",
      ", PUNCT\n",
      "and CCONJ\n",
      "the DET\n",
      "place NOUN\n",
      "is AUX\n",
      "always ADV\n",
      "VERY ADV\n",
      "Clean ADJ\n",
      ".... PUNCT\n",
      "Awesome ADJ\n",
      "job NOUN\n",
      "... PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc_1:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a1c0bece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DET\n",
      "view NOUN\n",
      "is AUX\n",
      "amazing ADJ\n",
      "but CCONJ\n",
      "the DET\n",
      "food NOUN\n",
      "is AUX\n",
      "all DET\n",
      "crap ADJ\n",
      ". PUNCT\n",
      "The DET\n",
      "wine NOUN\n",
      "is AUX\n",
      "over ADV\n",
      "priced VERB\n",
      "and CCONJ\n",
      "the DET\n",
      "food NOUN\n",
      "is AUX\n",
      "nothing PRON\n",
      "to PART\n",
      "write VERB\n",
      "about ADP\n",
      ". PUNCT\n",
      "The DET\n",
      "Server PROPN\n",
      "did AUX\n",
      "not PART\n",
      "even ADV\n",
      "know VERB\n",
      "how ADV\n",
      "to PART\n",
      "carry VERB\n",
      "plates NOUN\n",
      "or CCONJ\n",
      "serve VERB\n"
     ]
    }
   ],
   "source": [
    "for token in doc_3:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f82f0346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "know VERB\n",
      "that SCONJ\n",
      "I PRON\n",
      "'m AUX\n",
      "in ADP\n",
      "the DET\n",
      "minority NOUN\n",
      "... PUNCT\n",
      "but CCONJ\n",
      "I PRON\n",
      "really ADV\n",
      "do AUX\n",
      "not PART\n",
      "like VERB\n",
      "The DET\n",
      "Pretzel PROPN\n",
      "Shop PROPN\n",
      ". PUNCT\n",
      "My DET\n",
      "co NOUN\n",
      "- NOUN\n",
      "workers NOUN\n",
      "often ADV\n",
      "pick VERB\n",
      "up ADP\n",
      "pretzels NOUN\n",
      "from ADP\n",
      "here ADV\n",
      "for ADP\n",
      "birthdays NOUN\n",
      "or CCONJ\n",
      "other ADJ\n",
      "office NOUN\n",
      "events NOUN\n",
      ", PUNCT\n",
      "and CCONJ\n",
      "everyone PRON\n",
      "spends VERB\n",
      "the DET\n",
      "morning NOUN\n",
      "raving VERB\n",
      "about ADP\n",
      "how ADV\n",
      "delicious ADJ\n",
      "the DET\n",
      "pretzels NOUN\n",
      "are AUX\n",
      "and CCONJ\n",
      "how ADV\n",
      "they PRON\n",
      "can VERB\n",
      "not PART\n",
      "stop VERB\n",
      "eating VERB\n",
      "them PRON\n",
      ". PUNCT\n",
      "Am AUX\n",
      "I PRON\n",
      "missing VERB\n",
      "something PRON\n",
      "? PUNCT\n",
      "I PRON\n",
      "just ADV\n",
      "sit VERB\n",
      "quietly ADV\n",
      "at ADP\n",
      "my DET\n",
      "desk NOUN\n",
      "and CCONJ\n",
      "raise VERB\n",
      "an DET\n",
      "eyebrow NOUN\n",
      "at ADP\n",
      "all DET\n",
      "this DET\n",
      "chatter NOUN\n",
      ". PUNCT\n",
      "I PRON\n",
      "'ve AUX\n",
      "had AUX\n",
      "them PRON\n",
      "at ADP\n",
      "least ADJ\n",
      "5 NUM\n",
      "- SYM\n",
      "8 NUM\n",
      "times NOUN\n",
      "and CCONJ\n",
      "can VERB\n",
      "not PART\n",
      "say VERB\n",
      "that SCONJ\n",
      "I PRON\n",
      "'ve AUX\n",
      "ever ADV\n",
      "enjoyed VERB\n",
      "them PRON\n",
      ". PUNCT\n",
      "The DET\n",
      "regular ADJ\n",
      "pretzels NOUN\n",
      "are AUX\n",
      "either CCONJ\n",
      "over ADP\n",
      "or CCONJ\n",
      "under ADP\n",
      "salted VERB\n",
      "( PUNCT\n",
      "not PART\n",
      "sure ADJ\n",
      "if SCONJ\n",
      "that DET\n",
      "is AUX\n",
      "a DET\n",
      "preference NOUN\n",
      "the DET\n",
      "customer NOUN\n",
      "can VERB\n",
      "select VERB\n",
      "or CCONJ\n",
      "if SCONJ\n",
      "it PRON\n",
      "'s AUX\n",
      "just ADV\n",
      "random ADJ\n",
      ") PUNCT\n",
      ", PUNCT\n",
      "and CCONJ\n",
      "the DET\n",
      "cinnamon PROPN\n",
      "sugar NOUN\n",
      "just ADV\n",
      "taste VERB\n",
      "strange ADJ\n",
      ". PUNCT\n",
      "They PRON\n",
      "often ADV\n",
      "taste VERB\n",
      "very ADV\n",
      "soggy ADJ\n",
      ", PUNCT\n",
      "too ADV\n",
      ", PUNCT\n",
      "like SCONJ\n",
      "they PRON\n",
      "were AUX\n",
      "bagged VERB\n",
      "when ADV\n",
      "still ADV\n",
      "too ADV\n",
      "warm ADJ\n",
      ". PUNCT\n",
      "Occasionally ADV\n",
      ", PUNCT\n",
      "co NOUN\n",
      "- NOUN\n",
      "workers NOUN\n",
      "pick VERB\n",
      "- PUNCT\n",
      "up ADP\n",
      "the DET\n",
      "\" PUNCT\n",
      "cheese NOUN\n",
      "\" PUNCT\n",
      "sauce NOUN\n",
      ". PUNCT\n",
      "This DET\n",
      "stuff NOUN\n",
      "looks VERB\n",
      "like SCONJ\n",
      "a DET\n",
      "melted ADJ\n",
      "yellow ADJ\n",
      "crayon NOUN\n",
      ". PUNCT\n",
      "I PRON\n",
      "love VERB\n",
      "all DET\n",
      "types NOUN\n",
      "of ADP\n",
      "cheese NOUN\n",
      ", PUNCT\n",
      "but CCONJ\n",
      "this DET\n",
      "looks VERB\n",
      "and CCONJ\n",
      "tastes VERB\n",
      "terrible ADJ\n",
      ". PUNCT\n",
      "Yes INTJ\n",
      ", PUNCT\n",
      "I PRON\n",
      "'m AUX\n",
      "being AUX\n",
      "blasphemous ADJ\n",
      "towards ADP\n",
      "a DET\n",
      "Pittsburgh PROPN\n",
      "tradition NOUN\n",
      ", PUNCT\n",
      "I PRON\n",
      "know VERB\n",
      ", PUNCT\n",
      "but CCONJ\n",
      "I PRON\n",
      "'m AUX\n",
      "sorry ADJ\n",
      ", PUNCT\n",
      "I PRON\n",
      "'d VERB\n",
      "much ADV\n",
      "rather ADV\n",
      "get AUX\n",
      "a DET\n",
      "pretzel NOUN\n",
      "from ADP\n",
      "Hofbrauhaus PROPN\n",
      "or CCONJ\n",
      "even ADV\n",
      "Auntie PROPN\n",
      "Anne PROPN\n",
      "'s PART\n",
      "in ADP\n",
      "the DET\n",
      "mall NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc_4:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2941db4",
   "metadata": {},
   "source": [
    "# Method 3 Using Unigram Tagger trained with Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a5a7e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define new functions to tokenize the sentences generated\n",
    "def tokenizer(sents, num):\n",
    "    words = dict.fromkeys((i for i in range(num)), [])\n",
    "    for i in range(num):\n",
    "        words[i] = word_tokenize(sents[i])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d09f34ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Everyone at Auto Tint Express was super friend...\n",
      "1    For a waffle house this place is a little bett...\n",
      "2    I know that I'm in the minority... but I reall...\n",
      "3    The view is amazing but the food is all crap. ...\n",
      "4    We have used Happy Endings services twice now....\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(review_random_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "29312386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the sentences using the function definte previously\n",
    "words = tokenizer(review_random_df['text'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ec6e0056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/guangxushen/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cd09170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram tagger trained using Brown corpus\n",
    "def tagger_unigram(words, num):\n",
    "    from nltk.corpus import brown\n",
    "    brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "    ut = nltk.UnigramTagger(brown_tagged_sents)\n",
    "    #create a new dictionary & key\n",
    "    pos_2 = dict.fromkeys((i for i in range(num)), [])\n",
    "    #use i to iterate through the sentence index first, then use j to iterate through the words in sentences\n",
    "    for i,j in words.items():\n",
    "        pos_2[i] = ut.tag(j)\n",
    "    return pos_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2a26d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call unigram\n",
    "unigram_pos_tags = tagger_unigram(words, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a3e808bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results of unigram tagger:\n",
      "[('Everyone', None), ('at', 'IN'), ('Auto', None), ('Tint', None), ('Express', None), ('was', 'BEDZ'), ('super', 'JJ'), ('friendly', 'JJ'), (',', ','), ('they', 'PPSS'), ('helped', 'VBN'), ('me', 'PPO'), ('pick', 'VB'), ('a', 'AT'), ('good', 'JJ'), ('shade', 'NN'), ('to', 'TO'), ('go', 'VB'), ('with', 'IN'), ('for', 'IN'), ('my', 'PP$'), ('girls', 'NNS'), ('2014', None), ('corolla', None), ('.', '.'), ('Everything', 'PN'), ('came', 'VBD'), ('out', 'RP'), ('amazing', 'JJ'), ('and', 'CC'), ('the', 'AT'), ('waiting', 'VBG'), ('room', 'NN'), ('is', 'BEZ'), ('super', 'JJ'), ('nice', 'JJ'), ('.', '.'), ('Free', None), ('drinks', None), ('and', 'CC'), ('TV', 'NN'), ('to', 'TO'), ('watch', 'VB'), ('with', 'IN'), ('a', 'AT'), ('comfortable', 'JJ'), ('couch', 'NN'), ('.', '.'), ('Definitely', None), ('check', 'NN'), ('them', 'PPO'), ('out', 'RP'), ('for', 'IN'), ('any', 'DTI'), ('vehicle', 'NN'), ('you', 'PPSS'), ('have', 'HV'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResults of unigram tagger:\")\n",
    "print(unigram_pos_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3bfd03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88824abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
