{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ee24a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import spacy\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15500aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'data/reviewSelected100.json'\n",
    "os.path.exists(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d25f8f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8aoJJdKEO3ypoZNszpPu7Q</td>\n",
       "      <td>bGgAL09pxLnV_FFgR4ZADg</td>\n",
       "      <td>ZBE-H_aUlicix_9vUGQPIQ</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We had my Mother's Birthday Party here on 10/2...</td>\n",
       "      <td>2016-11-09 20:07:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J5NOCLdhuhor7USRhtYZ8w</td>\n",
       "      <td>pFCb-1j6oI3TDjr26h2cJQ</td>\n",
       "      <td>e-YnECeZNt8ngm0tu4X9mQ</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good Korean grill near Eaton Centre. The marin...</td>\n",
       "      <td>2015-12-05 05:06:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PXiLWAYRt3xnHaJ8MB4rzw</td>\n",
       "      <td>mEzc6LeTNiQgIVsq3poMbg</td>\n",
       "      <td>j7HO1YeMQGYo3KibMXZ5vg</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Was recommended to try this place by few peopl...</td>\n",
       "      <td>2014-10-11 05:16:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VrLarvxZYJm74yAqtpe9PQ</td>\n",
       "      <td>o-zUN2WEZgjQS7jnNsec0g</td>\n",
       "      <td>7e3PZzUpG5FYOTGt3O3ePA</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ambience: Would not expect something this nice...</td>\n",
       "      <td>2016-07-25 03:45:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1CUpidlVFprUCkApqzCmA</td>\n",
       "      <td>Wlx0iBXJvk4x0EeOt2Bz1Q</td>\n",
       "      <td>vuHzLZ7nAeT-EiecOkS5Og</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Absolutely the WORST pool company that I have ...</td>\n",
       "      <td>2016-04-11 18:49:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  8aoJJdKEO3ypoZNszpPu7Q  bGgAL09pxLnV_FFgR4ZADg  ZBE-H_aUlicix_9vUGQPIQ   \n",
       "1  J5NOCLdhuhor7USRhtYZ8w  pFCb-1j6oI3TDjr26h2cJQ  e-YnECeZNt8ngm0tu4X9mQ   \n",
       "2  PXiLWAYRt3xnHaJ8MB4rzw  mEzc6LeTNiQgIVsq3poMbg  j7HO1YeMQGYo3KibMXZ5vg   \n",
       "3  VrLarvxZYJm74yAqtpe9PQ  o-zUN2WEZgjQS7jnNsec0g  7e3PZzUpG5FYOTGt3O3ePA   \n",
       "4  C1CUpidlVFprUCkApqzCmA  Wlx0iBXJvk4x0EeOt2Bz1Q  vuHzLZ7nAeT-EiecOkS5Og   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      5       0      0     0   \n",
       "1      4       0      0     0   \n",
       "2      5       2      1     3   \n",
       "3      3       0      0     0   \n",
       "4      1      11      0     3   \n",
       "\n",
       "                                                text                date  \n",
       "0  We had my Mother's Birthday Party here on 10/2... 2016-11-09 20:07:25  \n",
       "1  Good Korean grill near Eaton Centre. The marin... 2015-12-05 05:06:43  \n",
       "2  Was recommended to try this place by few peopl... 2014-10-11 05:16:15  \n",
       "3  Ambience: Would not expect something this nice... 2016-07-25 03:45:26  \n",
       "4  Absolutely the WORST pool company that I have ... 2016-04-11 18:49:11  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df = pd.read_json(data_path, lines=True,encoding = \"ISO-8859-1\")\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f88ba98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly generate five indexes\n",
    "review_random_idx = np.array(np.random.rand(5)*len(review_df), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88774c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract out those sentences \n",
    "review_random_df = review_df.iloc[review_random_idx]\n",
    "review_random_df = review_random_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69720ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Update: Seriously disappointed that my gel pol...\n",
      "1    Five stars hands down!\\n\\nI am a proud Canadia...\n",
      "2    I stopped into the Wok Box Fresh Asian Kitchen...\n",
      "3    The best pretzels in town!\\n\\nWhen any of my c...\n",
      "4    Just went there for the first time. I got the ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(review_random_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "243d0d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update: Seriously disappointed that my gel polish chipped and peeled after ONLY two weeks. I've never had gels last such a short period of time. All the gel manis I've gotten have lasted 3-4 weeks, usually leaning on the 4 week end. Not sure if it's their application process or the quality of their products, but sadly I won't be returning now with gels only lasting 2 weeks.\n"
     ]
    }
   ],
   "source": [
    "print(review_random_df['text'].loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d65305",
   "metadata": {},
   "source": [
    "# Method 1 using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "242dc8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/guangxushen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/guangxushen/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a79ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the text\n",
    "review_random_df['tokenize'] = review_random_df['text'].apply(nltk.word_tokenize)\n",
    "#Tag those individual tokens respectively\n",
    "review_random_df['pos_tag'] = review_random_df['tokenize'].apply(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "915e135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the output as the resulting json\n",
    "review_random_df.to_json(r'output/reviewTagging5_1.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7707b74",
   "metadata": {},
   "source": [
    "# Method 2 using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1517b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ac33a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(review_random_df['text'].to_string())\n",
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92027360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Update: Seriously disappointed that my gel pol...\n",
      "1    Five stars hands down!\\n\\nI am a proud Canadia...\n",
      "2    I stopped into the Wok Box Fresh Asian Kitchen...\n",
      "3    The best pretzels in town!\\n\\nWhen any of my c...\n",
      "4    Just went there for the first time. I got the ...\n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "sentences = list(doc.sents)\n",
    "#print(len(sentences))\n",
    "# doc.to_json(r'output/reviewTagging5_2_doc.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16519d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get each of the line ready for pos tagging\n",
    "doc_0 = nlp(review_random_df.loc[0]['text'])\n",
    "doc_1 = nlp(review_random_df.loc[1]['text'])\n",
    "doc_2 = nlp(review_random_df.loc[2]['text'])\n",
    "doc_3 = nlp(review_random_df.loc[3]['text'])\n",
    "doc_4 = nlp(review_random_df.loc[4]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "519af20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five stars hands down!\n",
      "\n",
      "I am a proud Canadian and have had my fair share of poutines, but last night I felt like my taste buds had finally been awakened.\n",
      "\n",
      "Word on the street is they are getting a food truck! Look out Food Truck Frenzy, things are about to get cheesy!\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#checking of the sentence obtained\n",
    "print(doc_1)\n",
    "sentence_1 = list(doc_1.sents)\n",
    "print(len(sentence_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cec1bb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "POS Tag for sentence 1\n",
      "[['Update', 'NOUN'], [':', 'PUNCT'], ['Seriously', 'ADV'], ['disappointed', 'VERB'], ['that', 'SCONJ'], ['my', 'PRON'], ['gel', 'NOUN'], ['polish', 'NOUN'], ['chipped', 'VERB'], ['and', 'CCONJ'], ['peeled', 'VERB'], ['after', 'ADP'], ['ONLY', 'ADV'], ['two', 'NUM'], ['weeks', 'NOUN'], ['.', 'PUNCT'], ['I', 'PRON'], [\"'ve\", 'AUX'], ['never', 'ADV'], ['had', 'VERB'], ['gels', 'NOUN'], ['last', 'VERB'], ['such', 'DET'], ['a', 'DET'], ['short', 'ADJ'], ['period', 'NOUN'], ['of', 'ADP'], ['time', 'NOUN'], ['.', 'PUNCT'], ['All', 'DET'], ['the', 'DET'], ['gel', 'NOUN'], ['manis', 'NOUN'], ['I', 'PRON'], [\"'ve\", 'AUX'], ['gotten', 'VERB'], ['have', 'AUX'], ['lasted', 'VERB'], ['3', 'NUM'], ['-', 'SYM'], ['4', 'NUM'], ['weeks', 'NOUN'], [',', 'PUNCT'], ['usually', 'ADV'], ['leaning', 'VERB'], ['on', 'ADP'], ['the', 'DET'], ['4', 'NUM'], ['week', 'NOUN'], ['end', 'NOUN'], ['.', 'PUNCT'], ['Not', 'PART'], ['sure', 'ADJ'], ['if', 'SCONJ'], ['it', 'PRON'], [\"'s\", 'AUX'], ['their', 'PRON'], ['application', 'NOUN'], ['process', 'NOUN'], ['or', 'CCONJ'], ['the', 'DET'], ['quality', 'NOUN'], ['of', 'ADP'], ['their', 'PRON'], ['products', 'NOUN'], [',', 'PUNCT'], ['but', 'CCONJ'], ['sadly', 'ADV'], ['I', 'PRON'], ['wo', 'AUX'], [\"n't\", 'PART'], ['be', 'AUX'], ['returning', 'VERB'], ['now', 'ADV'], ['with', 'ADP'], ['gels', 'NOUN'], ['only', 'ADV'], ['lasting', 'VERB'], ['2', 'NUM'], ['weeks', 'NOUN'], ['.', 'PUNCT']]\n",
      "===================\n",
      "POS Tag for sentence 2\n",
      "[['Five', 'NUM'], ['stars', 'NOUN'], ['hands', 'VERB'], ['down', 'ADP'], ['!', 'PUNCT'], ['\\n\\n', 'SPACE'], ['I', 'PRON'], ['am', 'AUX'], ['a', 'DET'], ['proud', 'ADJ'], ['Canadian', 'PROPN'], ['and', 'CCONJ'], ['have', 'AUX'], ['had', 'VERB'], ['my', 'PRON'], ['fair', 'ADJ'], ['share', 'NOUN'], ['of', 'ADP'], ['poutines', 'NOUN'], [',', 'PUNCT'], ['but', 'CCONJ'], ['last', 'ADJ'], ['night', 'NOUN'], ['I', 'PRON'], ['felt', 'VERB'], ['like', 'ADP'], ['my', 'PRON'], ['taste', 'NOUN'], ['buds', 'NOUN'], ['had', 'AUX'], ['finally', 'ADV'], ['been', 'AUX'], ['awakened', 'VERB'], ['.', 'PUNCT'], ['\\n\\n', 'SPACE'], ['Word', 'NOUN'], ['on', 'ADP'], ['the', 'DET'], ['street', 'NOUN'], ['is', 'AUX'], ['they', 'PRON'], ['are', 'AUX'], ['getting', 'VERB'], ['a', 'DET'], ['food', 'NOUN'], ['truck', 'NOUN'], ['!', 'PUNCT'], ['Look', 'VERB'], ['out', 'ADP'], ['Food', 'PROPN'], ['Truck', 'PROPN'], ['Frenzy', 'PROPN'], [',', 'PUNCT'], ['things', 'NOUN'], ['are', 'AUX'], ['about', 'ADJ'], ['to', 'PART'], ['get', 'VERB'], ['cheesy', 'ADJ'], ['!', 'PUNCT']]\n",
      "===================\n",
      "POS Tag for sentence 3\n",
      "[['I', 'PRON'], ['stopped', 'VERB'], ['into', 'ADP'], ['the', 'DET'], ['Wok', 'PROPN'], ['Box', 'PROPN'], ['Fresh', 'PROPN'], ['Asian', 'PROPN'], ['Kitchen', 'PROPN'], ['the', 'DET'], ['other', 'ADJ'], ['night', 'NOUN'], ['on', 'ADP'], ['their', 'PRON'], ['second', 'ADJ'], ['day', 'NOUN'], ['of', 'ADP'], ['business', 'NOUN'], ['.', 'PUNCT'], ['I', 'PRON'], ['met', 'VERB'], ['the', 'DET'], ['husband', 'NOUN'], ['and', 'CCONJ'], ['wife', 'NOUN'], ['owners', 'NOUN'], ['and', 'CCONJ'], ['they', 'PRON'], ['seemed', 'VERB'], ['like', 'INTJ'], ['really', 'ADV'], ['nice', 'ADJ'], ['people', 'NOUN'], ['that', 'DET'], ['care', 'VERB'], ['about', 'ADP'], ['their', 'PRON'], ['customers', 'NOUN'], ['.', 'PUNCT'], ['The', 'DET'], ['Wok', 'PROPN'], ['Box', 'PROPN'], ['is', 'AUX'], ['a', 'DET'], ['Canadian', 'PROPN'], ['based', 'VERB'], ['Franchisor', 'PROPN'], ['.', 'PUNCT'], ['This', 'DET'], ['is', 'AUX'], ['the', 'DET'], ['second', 'ADJ'], ['Franchise', 'PROPN'], ['to', 'PART'], ['open', 'VERB'], ['in', 'ADP'], ['the', 'DET'], ['US', 'PROPN'], ['.', 'PUNCT'], ['\\n\\n', 'SPACE'], ['My', 'PRON'], ['wife', 'NOUN'], ['ordered', 'VERB'], ['this', 'DET'], ['noodle', 'NOUN'], ['bowl', 'NOUN'], ['that', 'DET'], ['we', 'PRON'], ['shared', 'VERB'], ['and', 'CCONJ'], ['I', 'PRON'], ['ordered', 'VERB'], ['some', 'DET'], ['spring', 'NOUN'], ['rolls', 'NOUN'], ['which', 'DET'], ['were', 'AUX'], ['served', 'VERB'], ['with', 'ADP'], ['plum', 'NOUN'], ['sauce', 'NOUN'], ['.', 'PUNCT'], ['The', 'DET'], ['noodle', 'NOUN'], ['bowl', 'NOUN'], ['was', 'AUX'], ['delicious', 'ADJ'], ['.', 'PUNCT'], ['The', 'DET'], ['eggrolls', 'NOUN'], ['came', 'VERB'], ['out', 'ADP'], [' ', 'SPACE'], ['a', 'DET'], ['little', 'ADJ'], ['frozen', 'ADJ'], ['the', 'DET'], ['first', 'ADJ'], ['time', 'NOUN'], ['around', 'ADV'], ['.', 'PUNCT'], ['But', 'CCONJ'], ['that', 'DET'], ['was', 'AUX'], ['quickly', 'ADV'], ['rectified', 'VERB'], ['by', 'ADP'], ['the', 'DET'], ['owners', 'NOUN'], ['.', 'PUNCT'], ['I', 'PRON'], ['really', 'ADV'], ['enjoyed', 'VERB'], ['the', 'DET'], ['plum', 'NOUN'], ['sauce', 'NOUN'], ['as', 'ADP'], ['opposed', 'VERB'], ['to', 'ADP'], ['the', 'DET'], ['omnipresent', 'ADJ'], ['orange', 'NOUN'], ['sauce', 'NOUN'], ['.', 'PUNCT'], ['\\n\\n', 'SPACE'], ['They', 'PRON'], ['have', 'VERB'], ['a', 'DET'], ['couple', 'NOUN'], ['of', 'ADP'], ['kinks', 'NOUN'], ['to', 'PART'], ['work', 'VERB'], ['out', 'ADP'], ['.', 'PUNCT'], ['But', 'CCONJ'], ['the', 'DET'], ['place', 'NOUN'], ['was', 'AUX'], ['comfortable', 'ADJ'], ['and', 'CCONJ'], ['inviting', 'VERB'], ['with', 'ADP'], ['a', 'DET'], ['self', 'NOUN'], ['serve', 'NOUN'], ['soda', 'NOUN'], ['machine', 'NOUN'], ['.', 'PUNCT'], ['They', 'PRON'], ['have', 'VERB'], ['a', 'DET'], ['lot', 'NOUN'], ['of', 'ADP'], ['different', 'ADJ'], ['entrees', 'NOUN'], ['from', 'ADP'], ['all', 'ADV'], ['over', 'ADP'], ['Asia', 'PROPN'], [';', 'PUNCT'], ['This', 'DET'], ['includes', 'VERB'], ['asian', 'ADJ'], [\"po'boy\", 'PROPN'], ['sandwiches', 'NOUN'], ['.', 'PUNCT'], ['They', 'PRON'], ['are', 'AUX'], ['a', 'DET'], ['couple', 'NOUN'], ['of', 'ADP'], ['bucks', 'NOUN'], ['more', 'ADJ'], ['than', 'SCONJ'], ['the', 'DET'], ['chinese', 'ADJ'], ['restaurant', 'NOUN'], ['we', 'PRON'], ['normally', 'ADV'], ['go', 'VERB'], ['to', 'PART'], ['but', 'CCONJ'], ['it', 'PRON'], [\"'s\", 'AUX'], ['a', 'DET'], ['nice', 'ADJ'], ['change', 'NOUN'], ['of', 'ADP'], ['pace', 'NOUN'], ['.', 'PUNCT'], ['It', 'PRON'], [\"'s\", 'AUX'], ['always', 'ADV'], ['good', 'ADJ'], ['to', 'PART'], ['have', 'VERB'], ['options', 'NOUN'], ['.', 'PUNCT'], ['The', 'DET'], ['attached', 'ADJ'], ['link', 'NOUN'], ['is', 'AUX'], ['a', 'DET'], ['trade', 'NOUN'], ['article', 'NOUN'], ['which', 'DET'], ['gives', 'VERB'], ['you', 'PRON'], ['some', 'DET'], ['more', 'ADJ'], ['information', 'NOUN'], [':', 'PUNCT'], ['\\n', 'SPACE'], ['http://www.restaurantnews.com/wok-box-fresh-asian-kitchen-opens-flagship-scottsdale-az-location/', 'PROPN']]\n",
      "===================\n",
      "POS Tag for sentence 4\n",
      "[['The', 'DET'], ['best', 'ADJ'], ['pretzels', 'NOUN'], ['in', 'ADP'], ['town', 'NOUN'], ['!', 'PUNCT'], ['\\n\\n', 'SPACE'], ['When', 'ADV'], ['any', 'DET'], ['of', 'ADP'], ['my', 'PRON'], ['coworkers', 'NOUN'], ['has', 'VERB'], ['to', 'PART'], ['go', 'VERB'], ['near', 'SCONJ'], ['the', 'DET'], ['South', 'PROPN'], ['Side', 'PROPN'], ['they', 'PRON'], ['stop', 'VERB'], ['by', 'ADP'], ['and', 'CCONJ'], ['get', 'VERB'], ['a', 'DET'], ['big', 'ADJ'], ['bag', 'NOUN'], ['of', 'ADP'], ['pretzels', 'NOUN'], ['to', 'PART'], ['share', 'VERB'], ['with', 'ADP'], ['everyone', 'PRON'], ['in', 'ADP'], ['the', 'DET'], ['office', 'NOUN'], ['.', 'PUNCT'], ['\\n', 'SPACE'], ['Not', 'PART'], ['only', 'ADV'], ['you', 'PRON'], ['get', 'AUX'], ['salted', 'VERB'], ['or', 'CCONJ'], ['unsalted', 'ADJ'], ['pretzels', 'NOUN'], ['but', 'CCONJ'], ['they', 'PRON'], ['also', 'ADV'], ['have', 'VERB'], ['stuffed', 'VERB'], ['pretzels', 'NOUN'], ['and', 'CCONJ'], ['even', 'ADV'], ['pizza', 'NOUN'], ['pretzels', 'NOUN'], ['.', 'PUNCT'], ['A', 'DET'], ['great', 'ADJ'], ['place', 'NOUN'], ['to', 'PART'], ['stop', 'VERB'], ['by', 'ADP'], ['for', 'ADP'], ['a', 'DET'], ['quick', 'ADJ'], ['and', 'CCONJ'], ['economical', 'ADJ'], ['lunch', 'NOUN'], ['/', 'SYM'], ['dinner', 'NOUN'], ['/', 'SYM'], ['snack', 'NOUN'], ['.', 'PUNCT'], ['\\n', 'SPACE'], ['Here', 'ADV'], ['you', 'PRON'], ['get', 'VERB'], ['quality', 'NOUN'], ['and', 'CCONJ'], ['price', 'NOUN'], ['.', 'PUNCT'], ['Do', 'AUX'], [\"n't\", 'PART'], ['be', 'AUX'], ['fooled', 'VERB'], ['by', 'ADP'], ['imitations', 'NOUN'], ['.', 'PUNCT'], ['This', 'DET'], ['place', 'NOUN'], ['has', 'AUX'], ['been', 'VERB'], ['open', 'ADJ'], ['for', 'ADP'], ['decades', 'NOUN'], [';', 'PUNCT'], ['there', 'PRON'], [\"'s\", 'AUX'], ['a', 'DET'], ['reason', 'NOUN'], ['for', 'ADP'], ['that', 'DET'], ['.', 'PUNCT']]\n",
      "===================\n",
      "POS Tag for sentence 5\n",
      "[['Just', 'ADV'], ['went', 'VERB'], ['there', 'ADV'], ['for', 'ADP'], ['the', 'DET'], ['first', 'ADJ'], ['time', 'NOUN'], ['.', 'PUNCT'], ['I', 'PRON'], ['got', 'VERB'], ['the', 'DET'], ['braised', 'VERB'], ['beef', 'NOUN'], ['banh', 'NOUN'], ['mi', 'NOUN'], ['sandwich', 'NOUN'], ['and', 'CCONJ'], ['it', 'PRON'], ['was', 'VERB'], ['delicious', 'ADJ'], ['.', 'PUNCT'], ['I', 'PRON'], [\"'m\", 'AUX'], ['going', 'VERB'], ['back', 'ADV'], ['tomorrow', 'NOUN'], ['for', 'ADP'], ['lunch', 'NOUN'], ['!', 'PUNCT'], ['The', 'DET'], ['service', 'NOUN'], ['was', 'AUX'], ['quick', 'ADJ'], ['even', 'ADV'], ['though', 'SCONJ'], ['there', 'PRON'], ['was', 'VERB'], ['a', 'DET'], ['fairly', 'ADV'], ['long', 'ADJ'], ['line', 'NOUN'], ['.', 'PUNCT'], ['Highly', 'ADV'], ['recommend', 'VERB'], ['.', 'PUNCT']]\n"
     ]
    }
   ],
   "source": [
    "result_list = [doc_0,doc_1,doc_2,doc_3,doc_4]\n",
    "counter = 0\n",
    "for element in result_list:\n",
    "    counter += 1\n",
    "    final_result = []\n",
    "    for token in element:\n",
    "        result = []\n",
    "        result.append(token.text)\n",
    "        result.append(token.pos_)\n",
    "        final_result.append(result)\n",
    "    print(\"===================\")\n",
    "    print(\"POS Tag for sentence\", str(counter))\n",
    "    print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f59739",
   "metadata": {},
   "source": [
    "# Method 3 Using Unigram Tagger trained with Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e46f5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define new functions to tokenize the sentences generated\n",
    "def tokenizer(sents, num):\n",
    "    words = dict.fromkeys((i for i in range(num)), [])\n",
    "    for i in range(num):\n",
    "        words[i] = word_tokenize(sents[i])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d09f34ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Update: Seriously disappointed that my gel pol...\n",
      "1    Five stars hands down!\\n\\nI am a proud Canadia...\n",
      "2    I stopped into the Wok Box Fresh Asian Kitchen...\n",
      "3    The best pretzels in town!\\n\\nWhen any of my c...\n",
      "4    Just went there for the first time. I got the ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(review_random_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29312386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the sentences using the function definte previously\n",
    "words = tokenizer(review_random_df['text'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec6e0056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/guangxushen/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd09170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram tagger trained with Brown corpus dataset\n",
    "def tagger_unigram(words, num):\n",
    "    from nltk.corpus import brown\n",
    "    brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "    unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "    #create a set of dictionary & key for words and taggs\n",
    "    pos_2 = dict.fromkeys((i for i in range(num)), [])\n",
    "    #use i to iterate through the sentence index first, then use j to iterate through the words in sentences\n",
    "    for sentence,word in words.items():\n",
    "        pos_2[sentence] = unigram_tagger.tag(word)\n",
    "    return pos_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a26d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call unigram\n",
    "unigram_pos_tags = tagger_unigram(words, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3e808bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results of unigram tagger:\n",
      "[('Update', None), (':', ':'), ('Seriously', None), ('disappointed', None), ('that', 'CS'), ('my', 'PP$'), ('gel', None), ('polish', None), ('chipped', 'VBD'), ('and', 'CC'), ('peeled', None), ('after', 'IN'), ('ONLY', None), ('two', 'CD'), ('weeks', 'NNS'), ('.', '.'), ('I', 'PPSS'), (\"'ve\", None), ('never', 'RB'), ('had', 'HVD'), ('gels', None), ('last', 'AP'), ('such', 'JJ'), ('a', 'AT'), ('short', 'JJ'), ('period', 'NN'), ('of', 'IN'), ('time', 'NN'), ('.', '.'), ('All', 'ABN'), ('the', 'AT'), ('gel', None), ('manis', None), ('I', 'PPSS'), (\"'ve\", None), ('gotten', 'VBN'), ('have', 'HV'), ('lasted', 'VBD'), ('3-4', None), ('weeks', 'NNS'), (',', ','), ('usually', 'RB'), ('leaning', None), ('on', 'IN'), ('the', 'AT'), ('4', 'CD'), ('week', 'NN'), ('end', 'NN'), ('.', '.'), ('Not', '*'), ('sure', 'JJ'), ('if', 'CS'), ('it', 'PPS'), (\"'s\", None), ('their', 'PP$'), ('application', 'NN'), ('process', 'NN'), ('or', 'CC'), ('the', 'AT'), ('quality', 'NN'), ('of', 'IN'), ('their', 'PP$'), ('products', 'NNS'), (',', ','), ('but', 'CC'), ('sadly', None), ('I', 'PPSS'), ('wo', None), (\"n't\", None), ('be', 'BE'), ('returning', 'VBG'), ('now', 'RB'), ('with', 'IN'), ('gels', None), ('only', 'AP'), ('lasting', 'VBG'), ('2', 'CD'), ('weeks', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResults of unigram tagger:\")\n",
    "print(unigram_pos_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3bfd03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88824abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
